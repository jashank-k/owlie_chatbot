{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae84dff",
      "metadata": {
        "id": "0ae84dff"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#v15 Scraper - Chat Context limited"
      ],
      "metadata": {
        "id": "1AMI26g7XrnT"
      },
      "id": "1AMI26g7XrnT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2dfb109",
      "metadata": {
        "id": "b2dfb109"
      },
      "outputs": [],
      "source": [
        "#Scraper makes sure all domains scraped fall within or are sub-domains under these higher level subdomains\n",
        "VALID_DOMAINS = [\n",
        "    'jindal.utdallas.edu', 'infosystems.utdallas.edu', 'fin.utdallas.edu',\n",
        "    'accounting.utdallas.edu', 'osim.utdallas.edu', 'om.utdallas.edu',\n",
        "    'marketing.utdallas.edu', 'sem.utdallas.edu', 'mba.utdallas.edu',\n",
        "    'execed.utdallas.edu', 'https://catalog.utdallas.edu/now/graduate/programs/jsom/', 'utdsolv.utdallas.edu', 'cometscommunity.utdallas.edu',\n",
        "    'www.utdallas.edu', 'jsom.utdallas.edu', 'https://jindal.utdallas.edu/faculty/', 'https://chairs.utdallas.edu/biographies/',\n",
        "    'https://policy.utdallas.edu/', 'http://deanofstudents.utdallas.edu/policies/'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These subdomains were found to only produce noisy data due to their design or dynamic nature, thus are specifically mentioned to be excluded\n",
        "EXCLUDE_DOMAINS = ['coursebook.utdallas.edu', 'news.utdallas.edu', 'math.utdallas.edu']"
      ],
      "metadata": {
        "id": "YRQ2BB07YkIM"
      },
      "id": "YRQ2BB07YkIM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dac58c6",
      "metadata": {
        "id": "5dac58c6"
      },
      "outputs": [],
      "source": [
        "#In case more links need to be added, use this\n",
        "extra_links = []\n",
        "if os.path.exists(\"extra_links.json\"):\n",
        "    with open(\"extra_links.json\", \"r\") as file:\n",
        "        extra_links = json.load(file)\n",
        "\n",
        "start_urls = list(set(seed_urls + extra_links))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_INTERVAL = 500  #Saves a copy of progress every 500 pages to prevent complete loss of data incase script stops prematurely\n",
        "\n",
        "scraped_count = 0\n",
        "visited = set()\n",
        "data = []\n",
        "\n",
        "def init_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    return webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "def is_valid_url(url):\n",
        "    if not url:\n",
        "        return False\n",
        "    parsed = urlparse(url)\n",
        "    domain = parsed.netloc\n",
        "    if any(excl in domain for excl in EXCLUDE_DOMAINS):\n",
        "        return False\n",
        "    if not any(domain.endswith(valid) for valid in VALID_DOMAINS):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def scrape_data(driver, url):\n",
        "    driver.get(url)\n",
        "    data = {'url': url, 'title': driver.title, 'text': ''}      #Data is scrapes as url, title and text based on their tags\n",
        "    try:\n",
        "        paragraphs = driver.find_elements(By.TAG_NAME, 'p')\n",
        "        data['text'] = ' '.join(p.text for p in paragraphs if p.text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {str(e)}\")               #Error handling during scraping\n",
        "        data['title'] = 'No title found'\n",
        "        data['text'] = 'No text found'\n",
        "    return data\n",
        "\n",
        "def get_links(driver):\n",
        "    links = []\n",
        "    try:\n",
        "        elements = driver.find_elements(By.TAG_NAME, 'a')\n",
        "        for e in elements:\n",
        "            href = e.get_attribute('href')\n",
        "            if href and is_valid_url(href):\n",
        "                links.append(href)\n",
        "    except:\n",
        "        pass\n",
        "    return links\n",
        "\n",
        "def save_checkpoint():\n",
        "    filename = f\"recursive_checkpoint_{scraped_count}.json\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "    print(f\"\\nğŸ’¾ Checkpoint saved: {filename}\")               #Checkpoint (every 500 pages saving confirmation)\n",
        "\n",
        "MAX_DEPTH = 4\n",
        "\n",
        "def crawl(seed_urls):\n",
        "    global scraped_count\n",
        "    driver = init_driver()\n",
        "    queue = deque([(url, 0) for url in seed_urls])\n",
        "\n",
        "    while queue:\n",
        "        url, depth = queue.popleft()\n",
        "        if url in visited or depth > MAX_DEPTH:\n",
        "            continue\n",
        "        visited.add(url)\n",
        "        page = scrape_data(driver, url)\n",
        "        data.append(page)\n",
        "        scraped_count += 1\n",
        "\n",
        "        links = get_links(driver)\n",
        "        new_links = [l for l in links if l not in visited]\n",
        "        queue.extend((link, depth + 1) for link in new_links)\n",
        "\n",
        "        print(f\"Scraped: {scraped_count:<5}, To Scrape: {len(queue):<5} ({len(new_links)} links added) | Depth: {depth} | {url}\")     #Real time log functionality to monitor scraper progress\n",
        "\n",
        "        if scraped_count % CHECKPOINT_INTERVAL == 0:\n",
        "            save_checkpoint()\n",
        "\n",
        "    driver.quit()\n",
        "    return data\n",
        "\n",
        "#Seed URLs\n",
        "sitemap_urls = [\n",
        "    'https://accounting.utdallas.edu/sitemap/', 'https://fin.utdallas.edu/sitemap/',\n",
        "    'https://osim.utdallas.edu/sitemap/', 'https://om.utdallas.edu/sitemap/',\n",
        "    'https://marketing.utdallas.edu/sitemap/', 'https://jindal.utdallas.edu/masters-programs/ms-mba-engineering-management/',\n",
        "    'https://sem.utdallas.edu/', 'https://mba.utdallas.edu/sitemap/',\n",
        "    'https://execed.utdallas.edu/sitemap/', 'https://jindal.utdallas.edu/masters-programs/double-degree/',\n",
        "    'https://infosystems.utdallas.edu/sitemap/', 'https://jindal.utdallas.edu/sitemap/', 'https://catalog.utdallas.edu/now/graduate/programs/jsom/',\n",
        "    'https://jindal.utdallas.edu/faculty/', 'https://chairs.utdallas.edu/biographies/', 'https://policy.utdallas.edu/', 'http://deanofstudents.utdallas.edu/policies/'\n",
        "]\n",
        "\n",
        "scraped_data = crawl(sitemap_urls)\n",
        "\n",
        "with open(\"recursive_full_scraped_data.json\", \"w\") as f:\n",
        "    json.dump(scraped_data, f, indent=2)\n",
        "\n",
        "print(\"\\nâœ… Full recursive scrape complete.\")                               #File saved name and comfirmation\n"
      ],
      "metadata": {
        "id": "0FLid-FZZJDw"
      },
      "id": "0FLid-FZZJDw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#v15 Data Cleaner"
      ],
      "metadata": {
        "id": "42A1KMCoabC3"
      },
      "id": "42A1KMCoabC3"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Load raw scraped data\n",
        "with open('recursive_full_scraped_data.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Regex patterns\n",
        "footer_patterns = [\n",
        "    re.compile(r\"The University of Texas at Dallas.*?Back to Top\", re.DOTALL | re.IGNORECASE),\n",
        "    re.compile(r\"Â©? ?\\d{4},? All rights reserved.*?\\(?972\\)?[-\\d\\s]+\", re.IGNORECASE),\n",
        "    re.compile(r\"View Published Works\", re.IGNORECASE),\n",
        "    re.compile(r\"Sitemap.*?\", re.IGNORECASE),\n",
        "    re.compile(r\"Last Updated.*?\", re.IGNORECASE),\n",
        "    re.compile(r\"Policy Form: UT Dallas Faculty Authored Textbook Approval Form.*?Return to Top\", re.DOTALL | re.IGNORECASE),\n",
        "    re.compile(r\"Naveen Jindal School of Management Business School\", re.IGNORECASE),\n",
        "    re.compile(r\"Last Updated:.*\", re.IGNORECASE),\n",
        "    re.compile(r\"Copyright Â© 2025, all rights reserved\\.\", re.IGNORECASE),\n",
        "    re.compile(r\"800 w campbell road,\", re.IGNORECASE),\n",
        "    re.compile(r\"richardson, tx 75080, usa Â· \\(972\\) 883-2705\", re.IGNORECASE),\n",
        "    re.compile(r\"the university of texas at dallas\", re.IGNORECASE),\n",
        "    re.compile(r\"- MA Arts, Technology, and Emerging Communication - MFA Arts, Technology, and Emerging Communication - PhD Audiology - AuD Bioinformatics and Computational Biology - MS Biomedical Engineering - MS Biomedical Engineering - PhD Biotechnology - MS Business Analytics - MS Business Administration\", re.IGNORECASE)\n",
        "]\n",
        "\n",
        "email_regex = re.compile(r\"[a-zA-Z0-9_.+-]+@utdallas\\.edu\")\n",
        "phone_regex = re.compile(r\"\\(\\d{3}\\) \\d{3}-\\d{4}\")\n",
        "room_regex = re.compile(r\"JSOM \\d+\\.\\d+\")\n",
        "\n",
        "def clean_title(title):\n",
        "    if not title:\n",
        "        return \"Untitled\"\n",
        "    title = re.sub(r\"Naveen Jindal School of Management\", \"\", title)\n",
        "    title = re.sub(r\"The University of Texas at Dallas\", \"\", title)\n",
        "    title = re.sub(r\"\\s*[\\|\\-]\\s*(UT Dallas|The University of Texas at Dallas)\", \"\", title)\n",
        "    title = re.sub(r\"[\\-|]\", \"\", title).strip()\n",
        "    return title.strip()\n",
        "\n",
        "def clean_text(text, faculty_name=\"\"):\n",
        "    for pattern in footer_patterns:\n",
        "        text = re.sub(pattern, \"\", text)\n",
        "\n",
        "    # Annotate and replace contact details\n",
        "    email_match = email_regex.search(text)\n",
        "    phone_match = phone_regex.search(text)\n",
        "    room_match = room_regex.search(text)\n",
        "\n",
        "    if email_match:\n",
        "        text = text.replace(email_match.group(0), f\"{faculty_name} email id: {email_match.group(0)}\")\n",
        "    if phone_match:\n",
        "        text = text.replace(phone_match.group(0), f\"{faculty_name} phone number: {phone_match.group(0)}\")\n",
        "    if room_match:\n",
        "        text = text.replace(room_match.group(0), f\"{faculty_name} office room number: {room_match.group(0)}\")\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Clean all data\n",
        "cleaned_data = []\n",
        "for entry in data:\n",
        "    title = clean_title(entry.get(\"title\", \"\"))\n",
        "    text = entry.get(\"text\", \"\")\n",
        "    faculty_name = title.split(\",\")[0].strip() if \",\" in title else title.strip()\n",
        "\n",
        "    cleaned_entry = {\n",
        "        \"url\": entry.get(\"url\", \"\").strip(),\n",
        "        \"title\": title,\n",
        "        \"text\": clean_text(text, faculty_name)\n",
        "    }\n",
        "    cleaned_data.append(cleaned_entry)\n",
        "\n",
        "# Cleaned output\n",
        "with open('v15_owlie_cleaned.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"âœ… All pages cleaned and saved to 'v15_owlie_cleaned.json'.\")\n"
      ],
      "metadata": {
        "id": "DBgmhpwbag4X"
      },
      "id": "DBgmhpwbag4X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#JSON to XLSX converter\n",
        "To manually check and ensure that the data is clean"
      ],
      "metadata": {
        "id": "rOYPOngGdMNz"
      },
      "id": "rOYPOngGdMNz"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('v15_owlie_cleaned.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.to_excel('v15_owlie_cleaned.xlsx', index=False)\n",
        "print(\"âœ… JSON successfully converted to Excel: v15_owlie_cleaned.xlsx\")"
      ],
      "metadata": {
        "id": "8LIkXO6SdIZg"
      },
      "id": "8LIkXO6SdIZg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}